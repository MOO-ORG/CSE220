**Big O Notation**

## ğŸ§  What Is Big O?

**Big O notation** is a way to **describe how fast an algorithm grows** â€” or how its running time (or space usage) increases as the size of the input increases.

It helps us answer questions like:

> â€œIf I double the input size, how much longer will my program take?â€

---

## ğŸ§© Why Do We Need Big O?

Letâ€™s say you and your friend write two sorting algorithms.

* Yours sorts 1,000 numbers in **1 second**.
* Your friendâ€™s sorts 1,000 numbers in **0.5 seconds**.

You might think your friendâ€™s is better â€” but what if you try **1 million numbers**?

* Yours takes 10 seconds.
* Your friendâ€™s takes 10 minutes.

Big O helps predict such behavior. Itâ€™s not about **actual time**, but about **how performance scales**.

---

## âš™ï¸ How We Measure

We measure the number of **basic operations** (like comparisons, additions, etc.) as the input size grows.

Letâ€™s call the input size **n**.

So Big O describes performance as a **function of n**, like:

* **O(1)** â†’ constant time
* **O(log n)** â†’ logarithmic time
* **O(n)** â†’ linear time
* **O(n log n)** â†’ â€œn log nâ€ time
* **O(nÂ²)** â†’ quadratic time
* **O(2â¿)** â†’ exponential time
* **O(n!)** â†’ factorial time (very slow)

---
# ğŸ§­ How to Analyze Big O

Hereâ€™s how to reason step-by-step:

1. **Look for loops**

   * One loop â†’ O(n)
   * Two nested loops â†’ O(nÂ²)

2. **Ignore constants**

   * O(2n) â†’ O(n)

3. **Keep the highest order term**

   * O(nÂ² + n) â†’ O(nÂ²)

4. **Think about input size**

   * A loop over 2D array â†’ O(n Ã— m)

---

## ğŸ” Letâ€™s See Examples



## ğŸ§® Example 1: Constant Time â€” **O(1)**

### Java

```java
void printFirst(int[] arr) {
    System.out.println(arr[0]);
}
```

### C++

```cpp
void printFirst(vector<int> arr) {
    cout << arr[0] << endl;
}
```

âœ… No matter the size of `arr`, this always takes **one step**.

---

## ğŸ§® Example 2: Linear Time â€” **O(n)**

### Java

```java
void printAll(int[] arr) {
    for (int i : arr) {
        System.out.println(i);
    }
}
```

### C++

```cpp
void printAll(vector<int> arr) {
    for (int i : arr) {
        cout << i << endl;
    }
}
```

âœ… The loop runs once for every element â†’ **linear growth**.

---

## ğŸ§® Example 3: Quadratic Time â€” **O(nÂ²)**

### Java

```java
void printPairs(int[] arr) {
    for (int i : arr) {
        for (int j : arr) {
            System.out.println(i + ", " + j);
        }
    }
}
```

### C++

```cpp
void printPairs(vector<int> arr) {
    for (int i : arr) {
        for (int j : arr) {
            cout << i << ", " << j << endl;
        }
    }
}
```

âœ… Double nested loops â†’ `n Ã— n` â†’ **O(nÂ²)**

---

## ğŸ§® Example 4: Logarithmic Time â€” **O(log n)**

### Java

```java
int binarySearch(int[] arr, int target) {
    int low = 0, high = arr.length - 1;
    while (low <= high) {
        int mid = (low + high) / 2;
        if (arr[mid] == target)
            return mid;
        else if (arr[mid] < target)
            low = mid + 1;
        else
            high = mid - 1;
    }
    return -1;
}
```

### C++

```cpp
int binarySearch(vector<int> arr, int target) {
    int low = 0, high = arr.size() - 1;
    while (low <= high) {
        int mid = (low + high) / 2;
        if (arr[mid] == target)
            return mid;
        else if (arr[mid] < target)
            low = mid + 1;
        else
            high = mid - 1;
    }
    return -1;
}
```

âœ… Each step cuts the data in half â†’ **O(log n)**

---

## ğŸ§® Example 5: n log n â€” **O(n log n)**

Typical for **efficient sorting** like Merge Sort or Quick Sort.

### Java (Merge Sort)

```java
void mergeSort(int[] arr) {
    if (arr.length <= 1) return;
    int mid = arr.length / 2;
    int[] left = Arrays.copyOfRange(arr, 0, mid);
    int[] right = Arrays.copyOfRange(arr, mid, arr.length);
    mergeSort(left);
    mergeSort(right);
    merge(arr, left, right);
}
```

âœ… Each recursion divides (`log n`) and merges (`n`) â†’ **O(n log n)**


---

## ğŸªœ How to Think About Growth

| Big O      | Input (n=10) | Input (n=100) | Relative Growth |
| ---------- | ------------ | ------------- | --------------- |
| O(1)       | 1            | 1             | Constant        |
| O(log n)   | 3            | 7             | Small growth    |
| O(n)       | 10           | 100           | Linear          |
| O(n log n) | 33           | 664           | Moderate        |
| O(nÂ²)      | 100          | 10,000        | Large           |
| O(2â¿)      | 1,024        | 1.27Ã—10Â³â°     | Explodes!       |

---

## ğŸ§­ Big O is About the **Worst Case**

When we say an algorithm is **O(n)**, we usually mean:

> â€œIn the **worst case**, it will take time proportional to n.â€

There are also:

* **Best case** (fastest)
* **Average case**
* **Worst case** (usually the one we care about)

---

## âš–ï¸ Space Complexity

Big O isnâ€™t only about time.
It also measures **space (memory)** usage.

Example:

```java
int[] copyArray(int[] arr) {
    int[] newArr = Arrays.copyOf(arr, arr.length);
    return newArr;
}
```

â¡ Extra array â†’ **O(n)** space

â†’ Space complexity: **O(n)**

---

## ğŸ§© Common Big O Values to Remember

| Big O      | Name         | Example                    |
| ---------- | ------------ | -------------------------- |
| O(1)       | Constant     | Accessing an array element |
| O(log n)   | Logarithmic  | Binary search              |
| O(n)       | Linear       | Loop through list          |
| O(n log n) | Linearithmic | Merge sort                 |
| O(nÂ²)      | Quadratic    | Nested loops               |
| O(2â¿)      | Exponential  | Recursive Fibonacci        |
| O(n!)      | Factorial    | Brute-force permutations   |

---

# âš–ï¸ Example Analysis

When analyzing code:

1. **Keep the largest term only**

   * O(nÂ² + n) â†’ **O(nÂ²)**
2. **Ignore constants**

   * O(2n) â†’ **O(n)**

Example:


```java
void example(int[] arr) {
    for (int i : arr) { // O(n)
        for (int j : arr) { // O(n)
            System.out.println(i + j);
        }
    }

    for (int k : arr) { // O(n)
        System.out.println(k);
    }
}
```

â¡ Total = O(nÂ² + n) = **O(nÂ²)**

---

## ğŸ§© Practice Questions

Try these! (Answers below)

 Whatâ€™s the Big O of these codes?


### ğŸ§­ Q1

```java
void func1(int[] arr) {
    System.out.println(arr[0]);
}
```

### ğŸ§­ Q2

```cpp
void func2(vector<int> arr) {
    for (int i : arr)
        cout << i << endl;
}
```

### ğŸ§­ Q3

```java
void func3(int[] arr) {
    for (int i : arr)
        for (int j : arr)
            System.out.println(i + " " + j);
}
```

### ğŸ§­ Q4

```cpp
void func4(vector<int> arr) {
    for (int i : arr)
        cout << i << endl;

    for (int i : arr)
        for (int j : arr)
            cout << i << ", " << j << endl;
}
```


### âœ… Answers

| Question | Big O     | Explanation          |
| -------- | --------- | -------------------- |
| Q1       | **O(1)**  | Constant work        |
| Q2       | **O(n)**  | One loop             |
| Q3       | **O(nÂ²)** | Two nested loops     |
| Q4       | **O(nÂ²)** | O(n) + O(nÂ²) â†’ O(nÂ²) |

---


## ğŸ’¡ Summary


## ğŸ” Letâ€™s Visualize the Growth

Imagine you have an input of size `n`.
Hereâ€™s roughly how many operations each complexity might take:

| n     | O(1) | O(log n) | O(n)  | O(n log n) | O(nÂ²)     | O(2â¿)     | O(n!)              |
| ----- | ---- | -------- | ----- | ---------- | --------- | --------- | ------------------ |
| 10    | 1    | 3        | 10    | 33         | 100       | 1 024     | 3.6 million        |
| 100   | 1    | 7        | 100   | 664        | 10 000    | 1.27Ã—10Â³â° | Too big to compute |
| 1 000 | 1    | 10       | 1 000 | 10 000     | 1 000 000 | 10Â³â°â°     | Unthinkable        |

ğŸ’¡ Notice:

* O(1) stays flat.
* O(log n) barely increases.
* O(n) grows steadily.
* O(nÂ²), O(2â¿), and O(n!) blow up quickly.

---

## ğŸ§  Intuition Summary

| Type           | Behavior            | Typical Example                | Rule of Thumb                    |
| -------------- | ------------------- | ------------------------------ | -------------------------------- |
| **O(1)**       | Constant            | Access element                 | Super fast â€” ideal!              |
| **O(log n)**   | Divide & conquer    | Binary search                  | Excellent for large data.        |
| **O(n)**       | Straight loop       | Linear search                  | Fine for most programs.          |
| **O(n log n)** | Smart sorting       | Merge sort                     | Good for sorting big data.       |
| **O(nÂ²)**      | Double loop         | Comparing all pairs            | OK only for small `n`.           |
| **O(2â¿)**      | Recursive explosion | Subset generation              | Avoid â€” grows too fast.          |
| **O(n!)**      | All permutations    | Traveling salesman brute force | Impractical â€” only for tiny `n`. |

---
#### Question â“ 

you asked:

> When `n = 10`, for O(log n), the value is 3.
> Where does that â€œ3â€ come from? What does it represent?

---

## ğŸ§© Step 1: What the Numbers Mean

These numbers donâ€™t represent **time in seconds** or **actual operations** â€” they are **relative growth values**.

They show *how many basic steps* (like comparisons, loops, or recursive calls) an algorithm might perform, compared to other Big O classes, when the input size is `n`.

So itâ€™s a **conceptual measure** of *how much work grows* â€” not an exact count.

---

## ğŸ§© Step 2: Why O(log n) â‰ˆ 3 when n = 10

That â€œ3â€ is roughly derived from the **logarithm of n** â€” usually using **base 2**, because many algorithms (like binary search) cut the data in half each step.

Letâ€™s calculate it:

$$
\log_2(10) \approx 3.32
$$

Thatâ€™s where the â€œ3â€ came from!

So if your algorithm is O(log n), and `n = 10`, it means it takes about 3 steps (because each step halves the data):

* Start: 10 elements
* Step 1 â†’ 5 elements
* Step 2 â†’ 2â€“3 elements
* Step 3 â†’ 1 element
  â†’ âœ… Done in about 3 steps.

---

### ğŸ” Another Example: n = 100

$$
\log_2(100) \approx 6.64
$$

â†’ So about **7 steps** to reduce 100 elements down to 1
(each step cutting the problem in half).

So that â€œ7â€ in the chart for O(log n) at n = 100 comes from logâ‚‚(100).

---

## ğŸ§© Step 3: What About the Other Columns?

Letâ€™s quickly interpret each column again for `n = 10`:

| Big O          | Meaning                  | What â€œvalueâ€ roughly means     |
| -------------- | ------------------------ | ------------------------------ |
| **O(1)**       | Constant                 | Always 1 step, regardless of n |
| **O(log n)**   | Halves data each step    | logâ‚‚(10) â‰ˆ 3 steps             |
| **O(n)**       | Processes every element  | 10 steps                       |
| **O(n log n)** | n Ã— logâ‚‚(n)              | 10 Ã— 3 = 30 (â‰ˆ33 in table)     |
| **O(nÂ²)**      | Nested loops             | 10 Ã— 10 = 100                  |
| **O(2â¿)**      | Doubles work per element | 2Â¹â° = 1,024                    |
| **O(n!)**      | All permutations         | 10! = 3,628,800                |

---

## ğŸ§  Step 4: What These Numbers *Represent Conceptually*

These numbers represent **how many units of work** an algorithm might perform **in proportion to the input size**.

They donâ€™t measure time directly (like milliseconds).
They help compare how quickly different algorithms â€œblow upâ€ as n grows.

---

## ğŸ” Example Intuition

Letâ€™s say you have three algorithms to process an array of size 10:

| Algorithm | Big O    | Approx Steps | Time (if 1 step = 1 ms) |
| --------- | -------- | ------------ | ----------------------- |
| A         | O(1)     | 1            | 1 ms                    |
| B         | O(log n) | 3            | 3 ms                    |
| C         | O(n)     | 10           | 10 ms                   |

Now if you increase `n` to 1,000:

| Algorithm | Big O    | Approx Steps | Time (if 1 step = 1 ms) |
| --------- | -------- | ------------ | ----------------------- |
| A         | O(1)     | 1            | 1 ms                    |
| B         | O(log n) | 10           | 10 ms                   |
| C         | O(n)     | 1,000        | 1,000 ms (1 sec)        |

See how O(1) and O(log n) barely change, but O(n) grows 100Ã—?

Thatâ€™s the **power of Big O notation** â€” it tells you how performance *scales* as inputs grow.

---
